{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db923a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Spark + Glue basic cheat sheet ===\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "# GlueContext â€” this is a part of AWS Glue SDK\n",
    "from awsglue.context import GlueContext \n",
    "from typing import Optional\n",
    "from awsglue.context import GlueContext\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "# Create SparkContext and GlueContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "glue_context = GlueContext(sc)\n",
    "\n",
    "# SparkSession is the main entry point for DataFrame and SQL API\n",
    "spark = glue_context.spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52fa0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Wrapper initialization ===\n",
    "class SparkWrapper:\n",
    "    def __init__(\n",
    "        self,\n",
    "        glue_context: GlueContext,\n",
    "        database_name: Optional[str] = None,\n",
    "    ):\n",
    "        self.spark = glue_context.spark_session  # SparkSession\n",
    "        self.database_name = database_name\n",
    "\n",
    "        if self.database_name:\n",
    "            self.spark.sql(f\"USE {self.database_name}\")\n",
    "\n",
    "    def sql(self, query: str) -> DataFrame:\n",
    "        # Execute Spark SQL (lazy)\n",
    "        return self.spark.sql(query)\n",
    "\n",
    "    def read_table(self, table_name: str, condition: Optional[str] = None) -> DataFrame:\n",
    "        # Read table from Glue Catalog\n",
    "        full_name = f\"{self.database_name}.{table_name}\" if self.database_name else table_name\n",
    "\n",
    "        if condition:\n",
    "            return self.sql(f\"SELECT * FROM {full_name} WHERE {condition}\")\n",
    "\n",
    "        return self.spark.read.table(full_name)\n",
    "    \n",
    "\n",
    "# Custom wrapper around Spark/Glue logic\n",
    "spark_wrapper = SparkWrapper(\n",
    "    glue_context=glue_context,\n",
    "    database_name=\"my_database\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40421aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Read table from Glue Catalog ===\n",
    "df_table = spark_wrapper.read_table(\"contacts\")\n",
    "# Reads table using spark.read.table(\"database.contacts\")\n",
    "\n",
    "\n",
    "# === Read table with filter condition ===\n",
    "df_filtered = spark_wrapper.read_table(\n",
    "    \"contacts\",\n",
    "    condition=\"is_deleted = false\"\n",
    ")\n",
    "# Executes Spark SQL with WHERE condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf9df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Execute Spark SQL ===\n",
    "df_sql = spark_wrapper.sql(\"\"\"\n",
    "    SELECT id, email\n",
    "    FROM contacts\n",
    "    WHERE email IS NOT NULL\n",
    "\"\"\")\n",
    "# Runs spark.sql() and returns DataFrame (lazy execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc3df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Read Athena view via JDBC ===\n",
    "df_athena = spark_wrapper.read_athena_view(\n",
    "    view_name=\"v_contacts\",\n",
    "    athena_output=\"s3://athena-results/\",\n",
    "    kms_key=\"kms-key-id\"\n",
    ")\n",
    "# Athena executes query and stores result in S3\n",
    "# Spark reads the result via JDBC\n",
    "\n",
    "\n",
    "# === Register Athena DataFrame as temp view ===\n",
    "df_athena.createOrReplaceTempView(\"v_contacts\")\n",
    "# Temp view can be used in Spark SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Read data from Aurora (PostgreSQL) ===\n",
    "df_aurora = spark_wrapper.read_aurora_table_to_df(\n",
    "    credentials=creds,\n",
    "    table_name=\"contacts\",\n",
    "    database_name=\"app_db\"\n",
    ")\n",
    "# JDBC read from Aurora into Spark DataFrame\n",
    "\n",
    "\n",
    "# === Update table data using join logic ===\n",
    "result_df = spark_wrapper.update_table_data_from_df(\n",
    "    new_data_df=new_df,\n",
    "    table_name=\"contacts\",\n",
    "    columns=[\"id\", \"email\", \"record_hash\", \"is_deleted\"],\n",
    "    id_column_names=[\"id\"]\n",
    ")\n",
    "# Left join new vs old data\n",
    "# New records are inserted\n",
    "# Old records are kept if record_hash is the same\n",
    "# is_deleted is updated from new data\n",
    "\n",
    "\n",
    "# === Action (triggers Spark execution) ===\n",
    "result_df.write.mode(\"overwrite\").saveAsTable(\"my_database.contacts\")\n",
    "# Actions like write/count trigger Spark job execution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
